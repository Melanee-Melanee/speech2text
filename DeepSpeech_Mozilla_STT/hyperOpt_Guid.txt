======================================== hyper optimization  ===================================

python3 DeepSpeech.py --train_files data/CV/en/clips/train2.csv --dev_files data/CV/en/clips/dev.csv --test_files data/CV/en/clips/test.csv --load_checkpoint_dir data/chkpnt_dir --save_checkpoint_dir data/tmp_chkp_save --export_dir data/tmp_chkp_load --n_hidden 2048 --epochs 1 --dropout_rate 0.40 --lm_alpha 0.75 --lm_beta 1.85  --learning_rate 0.00001 --train_batch_size 8 --dev_batch_size 8 --test_batch_size 4 --automatic_mixed_precision --train_cudnn True 


python3 DeepSpeech.py --train_files data/CV/en/clips/train2.csv --dev_files data/CV/en/clips/dev.csv --test_files data/CV/en/clips/test.csv --load_checkpoint_dir data/save_chknpt_2 --save_checkpoint_dir data/save_chknpt_2 --export_dir data/exprt_dir_2 --n_hidden 2048 --epochs 25 --dropout_rate 0.35 --lm_alpha 0.75 --lm_beta 1.85  --learning_rate 0.000001 --train_batch_size 8 --dev_batch_size 8 --test_batch_size 1 --automatic_mixed_precision --train_cudnn True --reduce_lr_on_plateau True --plateau_epochs 2 --plateau_reduction 0.0000002

best Result for this was





=======================================================  second Try ======================================

Changes for first Try : I'll try to reduce n_hidden . choose to have LR=0.00001  .ReduceLrOnPlateau=0.000001 plateau_epochs 2

python3 DeepSpeech.py --train_files data/CV/en/clips/train2.csv --dev_files data/CV/en/clips/dev.csv --test_files data/CV/en/clips/test.csv --load_checkpoint_dir data/save_chknpt_2 --save_checkpoint_dir data/save_chknpt_2 --export_dir data/exprt_dir_2 --n_hidden 512 --epochs 25 --dropout_rate 0.35 --lm_alpha 0.75 --lm_beta 1.85  --learning_rate 0.00001 --train_batch_size 8 --dev_batch_size 8 --test_batch_size 1 --automatic_mixed_precision --train_cudnn True --reduce_lr_on_plateau True --plateau_epochs 2 --plateau_reduction 0.0000002