{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note : this code is Heavily inspired by UDACITY course. and WE will update it more. This is first Version\n",
    "\n",
    "#### Note2 : this code is trained on turkish language. you can change it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to install these new libs. I used next line commands to install them in kaggle notebook\n",
    "'''\n",
    "all pip installs and commands : \n",
    "pip install pydub \n",
    "pip install python_speech_features\n",
    "!apt install -y ffmpeg\n",
    "'''\n",
    "\n",
    "# ignore warnings. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import librosa\n",
    "import pydub\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile # reading the wavfile\n",
    "\n",
    "# =========== < new import for approach 2 > ======================\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from python_speech_features import mfcc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import random\n",
    "import tensorflow\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Lambda)\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   \n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Activation\n",
    "# =========== </new import for approach 2> ======================\n",
    "\n",
    "# this is aparameter for audios.\n",
    "SIGNAL_RATE = 16000\n",
    "RNG_SEED = 123\n",
    "NumberOfSamples = 1823 # write number of samples here. number of data you have.\n",
    "#  this is the longest duration of an audio file.\n",
    "longest_y_length = 16000 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  this is the path to my files. change it to yours.\n",
    "PATH  = '/kaggle/input/trsst/tr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read an audio file you have many approaches. one of them is librosa library.\n",
    "# librosa approach to read audio.\n",
    "def read2(file):\n",
    "#     sound = pydub.AudioSegment.from_file(file, format=\"mp3\").set_frame_rate(8000)\n",
    "    sound = pydub.AudioSegment.from_file(file, format=\"mp3\")\n",
    "#     librosa.load ahs a duration property. accept just this much audio.\n",
    "    y, sr = librosa.load(file, sr=16000) # sound is \n",
    "    duration = librosa.core.get_duration(y=y , sr=sr )\n",
    "#     y, sr = librosa.load(file, sr=8000)\n",
    "    return sr, y, sound, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col(df, cols):\n",
    "    for col in cols:\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def prepare_dataset(df_address):\n",
    "    df =pd.read_csv(df_address, delimiter='\\t', encoding='utf-8')\n",
    "    drop_col(df, ['age', 'up_votes', 'client_id', 'accent', 'down_votes', 'gender'])\n",
    "    duration_list = []\n",
    "#     for path in list(df.path):\n",
    "#         y, sr = librosa.load(PATH + 'clips/' +  path, sr=16000) # sound is \n",
    "#         duration = librosa.core.get_duration(y=y , sr=sr )\n",
    "#         duration_list.append(duration)\n",
    "#     df['durations'] = duration_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you may be curious my csv file columns. We used Deep Speech Mozilla data. so Our csvs have these columns : \n",
    "*  age\n",
    "* up_votes\n",
    "* client_id\n",
    "* accent\n",
    "* down_votes\n",
    "* gender\n",
    "* sentece\n",
    "* path\n",
    "\n",
    "I just used 2 last columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "paths_list= list(devv.path)\n",
    "sr, y, sound, duration = read2(PATH + '/clips/' + paths_list[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next 2 cells are for setting our characters. we could use tensorflow tokenizer but we wrote it here. to be honest tensorflow approach is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will say in this panda file and it's specific column. how many unique character do you have.\n",
    "set(devv.sentence.apply(list).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_map.py\n",
    "# set(trainn.sentence.apply(list).sum()) # this code says to us how many unque characters do we have in sentences\n",
    "\n",
    "char_map_str = \"\"\"\n",
    "' 0\n",
    "<SPACE> 1\n",
    "a 2\n",
    "b 3\n",
    "c 4\n",
    "ç 5\n",
    "d 6\n",
    "e 7\n",
    "f 8\n",
    "g 9\n",
    "ğ 10\n",
    "h 11\n",
    "ı 12\n",
    "i 13\n",
    "j 14\n",
    "k 15\n",
    "l 16\n",
    "m 17\n",
    "n 18\n",
    "o 19\n",
    "ö 20\n",
    "p 21\n",
    "r 22\n",
    "s 23\n",
    "ş 24\n",
    "t 25\n",
    "u 26\n",
    "ü 27\n",
    "v 28\n",
    "y 29\n",
    "z 30\n",
    "A 31\n",
    "B 32\n",
    "C 33\n",
    "Ç 34\n",
    "D 35\n",
    "E 36\n",
    "F 37\n",
    "G 38\n",
    "Ğ 39\n",
    "H 40\n",
    "I 41\n",
    "İ 42\n",
    "J 43\n",
    "K 44\n",
    "L 45\n",
    "M 46\n",
    "N 47\n",
    "O 48\n",
    "Ö 49\n",
    "P 50\n",
    "R 51\n",
    "S 52\n",
    "Ş 53\n",
    "T 54\n",
    "U 55\n",
    "Ü 56\n",
    "V 57\n",
    "Y 58\n",
    "Z 59\n",
    ". 60\n",
    "! 61\n",
    "\" 62\n",
    "% 63\n",
    ", 64\n",
    "- 65\n",
    "? 66\n",
    "‘ 67\n",
    "� 68\n",
    "â 69\n",
    "w 70 \n",
    "W 71\n",
    "q 72\n",
    "Q 73\n",
    "x 74\n",
    "X 75\n",
    "î 76\n",
    "\"\"\"\n",
    "# the \"blank\" character is mapped to 28\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "    ch, index = line.split()\n",
    "    char_map[ch] = int(index)\n",
    "    index_map[int(index)+1] = ch\n",
    "print(index_map[2])\n",
    "index_map[2] = ' '\n",
    "print(index_map[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next Cells are for our important functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feat_dim(window, max_freq):\n",
    "    return int(0.001 * window * max_freq) + 1\n",
    "\n",
    "def conv_output_length(input_length, filter_size, border_mode, stride,\n",
    "                       dilation=1):\n",
    "    \"\"\" Compute the length of the output sequence after 1D convolution along\n",
    "        time. Note that this function is in line with the function used in\n",
    "        Convolution1D class from Keras.\n",
    "    Params:\n",
    "        input_length (int): Length of the input sequence.\n",
    "        filter_size (int): Width of the convolution kernel.\n",
    "        border_mode (str): Only support `same` or `valid`.\n",
    "        stride (int): Stride size used in 1D convolution.\n",
    "        dilation (int)\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert border_mode in {'same', 'valid'}\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "\n",
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "    Returns: \n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)\n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check \n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod \n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs\n",
    "\n",
    "\n",
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "#     asssume we don't need next line ==>  PATH + 'clips/' + filename\n",
    "    audio, sample_rate = librosa.load(PATH + 'clips/' + filename, sr=16000)\n",
    "#     audio = padarray(audio, 160000)\n",
    "    if audio.ndim >= 2:\n",
    "        audio = np.mean(audio, 1)\n",
    "    if max_freq is None:\n",
    "        max_freq = sample_rate / 2\n",
    "    if max_freq > sample_rate / 2:\n",
    "        raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                            \" sample rate\")\n",
    "    if step > window:\n",
    "        raise ValueError(\"step size must not be greater than window size\")\n",
    "    hop_length = int(0.001 * step * sample_rate)\n",
    "    fft_length = int(0.001 * window * sample_rate)\n",
    "    pxx, freqs = spectrogram(\n",
    "        audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "        hop_length=hop_length)\n",
    "    ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "#     next lines are replaced with prev lines\n",
    "#     with soundfile.SoundFile(filename) as sound_file:\n",
    "#         audio = sound_file.read(dtype='float32')\n",
    "#         sample_rate = sound_file.samplerate\n",
    "#         if audio.ndim >= 2:\n",
    "#             audio = np.mean(audio, 1)\n",
    "#         if max_freq is None:\n",
    "#             max_freq = sample_rate / 2\n",
    "#         if max_freq > sample_rate / 2:\n",
    "#             raise ValueError(\"max_freq must not be greater than half of \"\n",
    "#                              \" sample rate\")\n",
    "#         if step > window:\n",
    "#             raise ValueError(\"step size must not be greater than window size\")\n",
    "#         hop_length = int(0.001 * step * sample_rate)\n",
    "#         fft_length = int(0.001 * window * sample_rate)\n",
    "#         pxx, freqs = spectrogram(\n",
    "#             audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "#             hop_length=hop_length)\n",
    "#         ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "#     return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "\n",
    "\n",
    "def text_to_int_sequence(text):\n",
    "    \"\"\" Convert text to an integer sequence \"\"\"\n",
    "    int_sequence = []\n",
    "#     text = text.lower()\n",
    "#     print(text)\n",
    "    for c in text:\n",
    "#         print(c)\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    return int_sequence\n",
    "\n",
    "def int_sequence_to_text(int_sequence):\n",
    "    \"\"\" Convert an integer sequence to text \"\"\"\n",
    "    text = []\n",
    "    for c in int_sequence:\n",
    "        ch = index_map[c]\n",
    "        text.append(ch)\n",
    "    return text\n",
    "\n",
    "# this is the way to pad all the y of audios to same size.\n",
    "def padarray(A, size):\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')\n",
    "\n",
    "padded = padarray(y, 160000)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR CUSTOM DATA GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGenerator():\n",
    "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=26,\n",
    "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
    "        sort_by_duration=False):\n",
    "        \"\"\"\n",
    "        Params: \n",
    "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
    "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
    "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "                [0, max_freq] are returned (for spectrogram ONLY)\n",
    "            desc_file (str, optional): Path to a JSON-line file that contains\n",
    "                labels and paths to the audio files. If this is None, then\n",
    "                load metadata right away\n",
    "        \"\"\"\n",
    "\n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.mfcc_dim = mfcc_dim\n",
    "        self.feats_mean = np.zeros((self.feat_dim,))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        if desc_file is not None:\n",
    "            self.load_metadata_from_desc_file(desc_file)\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.max_duration=max_duration\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.sort_by_duration = sort_by_duration\n",
    "\n",
    "    def get_batch(self, partition):\n",
    "        \"\"\" Obtain a batch of train, validation, or test data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio_paths\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio_paths\n",
    "            cur_index = self.cur_valid_index\n",
    "            texts = self.valid_texts\n",
    "        elif partition == 'test':\n",
    "            audio_paths = self.test_audio_paths\n",
    "            cur_index = self.test_valid_index\n",
    "            texts = self.test_texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "        features = [self.normalize(self.featurize(a)) for a in \n",
    "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "        \n",
    "        # calculate necessary sizes\n",
    "#         calculate maximum length from spectogram features\n",
    "        max_length = max([features[i].shape[0] \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        \n",
    "#     calculate miximum length of sentences.\n",
    "        max_string_length = max([len(texts[cur_index+i]) \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "          \n",
    "        # initialize the arrays\n",
    "        X_data = np.zeros([self.minibatch_size, max_length, \n",
    "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
    "#         here we create all blanks for labels :). maybe you say why? because 1 * number_of_blank. (default is 1 * 28, because that code had 27 characters.)\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 77\n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])  \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # calculate X_data & input_length\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "\n",
    "            # calculate labels & label_length\n",
    "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    " \n",
    "        # return the arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data, \n",
    "                  'the_labels': labels, \n",
    "                  'input_length': input_length, \n",
    "                  'label_length': label_length \n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def shuffle_data_by_partition(self, partition):\n",
    "        \"\"\" Shuffle the training or validation data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "#             self.train_audio_paths, self.train_durations, self.train_texts = self.shuffle_data(\n",
    "#                 self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "            self.train_audio_paths, self.train_texts = self.shuffle_data(\n",
    "                self.train_audio_paths,  self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "#             self.valid_audio_paths, self.valid_durations, self.valid_texts = self.shuffle_data(\n",
    "#                 self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "            self.valid_audio_paths, self.valid_texts = self.shuffle_data(\n",
    "                self.valid_audio_paths, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def sort_data_by_duration(self, partition):\n",
    "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def next_train(self):\n",
    "        \"\"\" Obtain a batch of training data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_data_by_partition('train')\n",
    "            yield ret    \n",
    "\n",
    "    def next_valid(self):\n",
    "        \"\"\" Obtain a batch of validation data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_data_by_partition('valid')\n",
    "            yield ret\n",
    "\n",
    "    def next_test(self):\n",
    "        \"\"\" Obtain a batch of test data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('test')\n",
    "            self.cur_test_index += self.minibatch_size\n",
    "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
    "                self.cur_test_index = 0\n",
    "            yield ret\n",
    "\n",
    "#             this method is called in second Main step 2.0\n",
    "    def load_train_data(self, desc_file=PATH + \"train.tsv\"):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
    "        self.fit_train()\n",
    "#         I don't want to test this yet.\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('train')\n",
    "\n",
    "            # this method is called in third main step 3.0\n",
    "    def load_validation_data(self, desc_file=PATH + \"dev.tsv\"):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'dev')\n",
    "        #         I don't want to test this yet.\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('valid')\n",
    "\n",
    "    def load_test_data(self, desc_file=PATH + \"test.tsv\"):\n",
    "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
    "    \n",
    "#     2.3\n",
    "    def drop_col(df, cols):\n",
    "        for col in cols:\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#             2.2\n",
    "    def prepare_dataset(df_address):\n",
    "        df =pd.read_csv(df_address, delimiter='\\t',encoding='utf-8')\n",
    "        drop_col(df, ['age', 'up_votes', 'client_id', 'accent', 'down_votes', 'gender'])\n",
    "        duration_list = []\n",
    "        for path in list(df.path):\n",
    "            y, sr = librosa.load(PATH + 'clips/' +  path, sr=16000) # sound is \n",
    "            duration = librosa.core.get_duration(y=y , sr=sr )\n",
    "            duration_list.append(duration)\n",
    "        df['durations'] = duration_list\n",
    "        return df\n",
    "    \n",
    "    \n",
    "#     2.1\n",
    "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
    "        \"\"\" Read metadata from a tsv file\n",
    "            (possibly takes long, depending on the filesize)\n",
    "        Params:\n",
    "            desc_file (str):  Path to folder which contains all tsv files and clips folder\n",
    "            partition (str): One of 'train', 'dev' or 'test'\n",
    "        \"\"\"\n",
    "        audio_paths, durations, texts = [], [], []\n",
    "        # read tsv file\n",
    "# TEMP NEXT LINE IS BECOMING COMMENT\n",
    "#         df = prepare_dataset(desc_file)\n",
    "\n",
    "        if partition == 'train':\n",
    "            df = trainn\n",
    "            self.train_audio_paths = list(df.path)\n",
    "#             self.train_durations = list(df.durations)\n",
    "            self.train_texts = list(df.sentence)\n",
    "        elif partition == 'dev':\n",
    "            df = devv\n",
    "            self.valid_audio_paths = list(df.path)\n",
    "#             self.valid_durations = list(df.durations)\n",
    "            self.valid_texts = list(df.sentence)\n",
    "        elif partition == 'test':\n",
    "            self.test_audio_paths = df.path\n",
    "#             self.test_durations = df.durations\n",
    "            self.test_texts = df.sentence\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition to load metadata. \"\n",
    "             \"Must be train/validation/test\")\n",
    "            \n",
    "    \n",
    "    #2.4\n",
    "    def fit_train(self, k_samples=100):\n",
    "        \"\"\" Estimate the mean and std of the features from the training set\n",
    "        Params:\n",
    "            k_samples (int): Use this number of samples for estimation\n",
    "        \"\"\"\n",
    "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "        \n",
    "    def featurize(self, audio_clip):\n",
    "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
    "        Params:\n",
    "            audio_clip (str): Path to the audio clip\n",
    "        \"\"\"\n",
    "        if self.spectrogram:\n",
    "            return spectrogram_from_file(\n",
    "                audio_clip, step=self.step, window=self.window,\n",
    "                max_freq=self.max_freq)\n",
    "        else:\n",
    "#             I need t change this with librosa loading files.\n",
    "            audio, sample_rate = librosa.load(PATH + 'clips/' + audio_clip, sr=16000)\n",
    "#             audio = padarray(audio, 160000)\n",
    "#             (rate, sig) = wav.read(audio_clip)\n",
    "            return mfcc(audio, sample_rate, numcep=self.mfcc_dim)\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "        \"\"\" Center a feature using the mean and std\n",
    "        Params:\n",
    "            feature (numpy.ndarray): Feature to normalize\n",
    "        \"\"\"\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "\n",
    "#     def shuffle_data(self, a, d, t):\n",
    "    def shuffle_data(self, a, t):\n",
    "        \"\"\" Shuffle the data (called after making a complete pass through \n",
    "            training or validation data during the training process)\n",
    "        Params:\n",
    "            audio_paths (list): Paths to audio clips\n",
    "            durations (list): Durations of utterances for each audio clip\n",
    "            texts (list): Sentences uttered in each audio clip\n",
    "        \"\"\"\n",
    "        p = np.random.permutation(len(a))\n",
    "        a = [a[i] for i in p]\n",
    "#         d = [d[i] for i in p]\n",
    "        t = [t[i] for i in p]\n",
    "#         return a, d, t\n",
    "        return a, t\n",
    "\n",
    "\n",
    "    def sort_data(audio_paths, durations, texts):\n",
    "        \"\"\" Sort the data by duration \n",
    "        Params:\n",
    "            audio_paths (list): Paths to audio clips\n",
    "            durations (list): Durations of utterances for each audio clip\n",
    "            texts (list): Sentences uttered in each audio clip\n",
    "        \"\"\"\n",
    "        p = np.argsort(durations).tolist()\n",
    "        audio_paths = [audio_paths[i] for i in p]\n",
    "        durations = [durations[i] for i in p] \n",
    "        texts = [texts[i] for i in p]\n",
    "        return audio_paths, durations, texts\n",
    "\n",
    "    def vis_train_features(index=0):\n",
    "        \"\"\" Visualizing the data point in the training set at the supplied index\n",
    "        \"\"\"\n",
    "        # obtain spectrogram\n",
    "        audio_gen = AudioGenerator(spectrogram=True)\n",
    "        audio_gen.load_train_data()\n",
    "        vis_audio_path = audio_gen.train_audio_paths[index]\n",
    "        vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "        # obtain mfcc\n",
    "        audio_gen = AudioGenerator(spectrogram=False)\n",
    "        audio_gen.load_train_data()\n",
    "        vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "        # obtain text label\n",
    "        vis_text = audio_gen.train_texts[index]\n",
    "        # obtain raw audio\n",
    "        vis_raw_audio, _ = librosa.load(vis_audio_path)\n",
    "        # print total number of training examples\n",
    "        print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
    "        # return labels for plotting\n",
    "        return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
    "\n",
    "\n",
    "    def plot_raw_audio(vis_raw_audio):\n",
    "        # plot the raw audio signal\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        ax = fig.add_subplot(111)\n",
    "        steps = len(vis_raw_audio)\n",
    "        ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
    "        plt.title('Audio Signal')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_mfcc_feature(vis_mfcc_feature):\n",
    "        # plot the MFCC feature\n",
    "        fig = plt.figure(figsize=(12,5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "        plt.title('Normalized MFCC')\n",
    "        plt.ylabel('Time')\n",
    "        plt.xlabel('MFCC Coefficient')\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(im, cax=cax)\n",
    "        ax.set_xticks(np.arange(0, 26, 2), minor=False);\n",
    "        plt.show()\n",
    "\n",
    "    def plot_spectrogram_feature(vis_spectrogram_feature):\n",
    "        # plot the normalized spectrogram\n",
    "        fig = plt.figure(figsize=(12,5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "        plt.title('Normalized Spectrogram')\n",
    "        plt.ylabel('Time')\n",
    "        plt.xlabel('Frequency')\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(im, cax=cax)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def add_ctc_loss(input_to_softmax):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(\n",
    "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "        outputs=loss_out)\n",
    "    return model\n",
    "\n",
    "def train_model(input_to_softmax, \n",
    "                pickle_path,\n",
    "                save_model_path,\n",
    "                train_json='train_corpus.json',\n",
    "                valid_json='valid_corpus.json',\n",
    "                minibatch_size=20,\n",
    "                spectrogram=True,\n",
    "                mfcc_dim=26,\n",
    "                optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "                epochs=20,\n",
    "                verbose=1,\n",
    "                sort_by_duration=False,\n",
    "                max_duration=10.0):\n",
    "    \n",
    "    # create a class instance for obtaining batches of data\n",
    "    audio_gen = AudioGenerator(minibatch_size=minibatch_size, \n",
    "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
    "        sort_by_duration=sort_by_duration)\n",
    "    # add the training data to the generator\n",
    "    audio_gen.load_train_data()\n",
    "    audio_gen.load_validation_data()\n",
    "    # calculate steps_per_epoch\n",
    "    num_train_examples=len(audio_gen.train_audio_paths)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # calculate validation_steps\n",
    "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
    "    validation_steps = num_valid_samples//minibatch_size\n",
    "    \n",
    "    # add CTC loss to the NN specified in input_to_softmax\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "\n",
    "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "\n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "\n",
    "    # add checkpointer\n",
    "#     checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)\n",
    "\n",
    "    # train the model\n",
    "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps, verbose=verbose)\n",
    "\n",
    "    # save model loss\n",
    "    with open('results/'+pickle_path, 'wb') as f:\n",
    "        pickle.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAY\n",
    "### until here we created every thing we need to train our model.\n",
    "now you just need to set your model structure and send it to our Magical function\n",
    "\n",
    "#### Very simple RNN model for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dimesion is so important. be tedade kalame hayi ke addad kardam + blank\n",
    "def simple_rnn_model(input_dim, output_dim=78):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(output_dim, return_sequences=True, \n",
    "                 implementation=2, name='rnn')(input_data)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(simp_rnn)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to 26 if you would like to use MFCC features\n",
    "model_0 = simple_rnn_model(input_dim=161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_0,\n",
    "            pickle_path='model_0.pickle',\n",
    "            save_model_path='model_0.h5',\n",
    "            spectrogram=True) # Change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitbef378172e7e4093bc71f9761c8616f0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}