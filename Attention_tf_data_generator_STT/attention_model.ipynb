{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses attention mechanism to create an speech to text system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this 2 line are my csv and audio feature extracted files.\n",
    "# Path is csv file for audios and their texts\n",
    "# PATH2 is audio feature extracted files. you'll create it in this code.\n",
    "PATH  = '/kaggle/input/trsst/tr/'\n",
    "PATH2 = '/kaggle/input/tr-attention-features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to install these libraries. use these commands to install them.\n",
    "'''\n",
    "all pip installs and commands\n",
    "pip3 install pydub\n",
    "pip3 install python_speech_features\n",
    "apt-get install -y ffmpeg\n",
    "'''\n",
    "\n",
    "# ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may see duplicated lines. don't matter\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import librosa\n",
    "import pydub\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile # reading the wavfile\n",
    "\n",
    "# =========== < new import for approach 2 > ======================\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from python_speech_features import mfcc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import random\n",
    "import tensorflow\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Lambda)\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   \n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Activation\n",
    "# =========== </new import for approach 2> ======================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "\n",
    "# signal rate is a parameter for audios\n",
    "SIGNAL_RATE = 16000\n",
    "RNG_SEED = 123\n",
    "NumberOfSamples = 1823 # write number of samples here. number of data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little work with csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col(df, cols):\n",
    "    for col in cols:\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def prepare_dataset(df_address):\n",
    "    df =pd.read_csv(df_address, delimiter='\\t', encoding='utf-8')\n",
    "    drop_col(df, ['age', 'up_votes', 'client_id', 'accent', 'down_votes', 'gender'])\n",
    "    # If you need duration of each file just uncomment next lines\n",
    "#     duration_list = []\n",
    "#     for path in list(df.path):\n",
    "#         y, sr = librosa.load(PATH + 'clips/' +  path, sr=16000) # sound is \n",
    "#         duration = librosa.core.get_duration(y=y , sr=sr )\n",
    "#         duration_list.append(duration)\n",
    "#     df['durations'] = duration_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may change these lines\n",
    "#prepare validation and train data just for now\n",
    "partition='train'\n",
    "train_attention = prepare_dataset(PATH+ 'train' + '.tsv' )\n",
    "\n",
    "partition = 'dev'\n",
    "dev_attention = prepare_dataset(PATH+ 'dev' + '.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you may be curious my csv file columns. We used Deep Speech Mozilla data. so Our csvs have these columns : \n",
    "*  age\n",
    "* up_votes\n",
    "* client_id\n",
    "* accent\n",
    "* down_votes\n",
    "* gender\n",
    "* sentece\n",
    "* path\n",
    "\n",
    "I just used 2 last columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful funcitons to Preprocess Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the way to pad all the y of audios to same size.\n",
    "def padarray(A, size):\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')\n",
    "\n",
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "    Returns:\n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)  \n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs\n",
    "\n",
    "\n",
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "#     asssume we don't need next line ==>  PATH + 'clips/' + filename\n",
    "    audio, sample_rate = librosa.load(PATH + 'clips/' + filename, sr=16000, duration = 10)\n",
    "    audio = padarray(audio, 160000)\n",
    "    if audio.ndim >= 2:\n",
    "        audio = np.mean(audio, 1)\n",
    "    if max_freq is None:\n",
    "        max_freq = sample_rate / 2\n",
    "    if max_freq > sample_rate / 2:\n",
    "        raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                            \" sample rate\")\n",
    "    if step > window:\n",
    "        raise ValueError(\"step size must not be greater than window size\")\n",
    "    hop_length = int(0.001 * step * sample_rate)\n",
    "    fft_length = int(0.001 * window * sample_rate)\n",
    "    pxx, freqs = spectrogram(\n",
    "        audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "        hop_length=hop_length)\n",
    "#     print(pxx)\n",
    "#     print(freqs)\n",
    "    ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "\n",
    "def calc_mfcc(filename):\n",
    "    dim=26\n",
    "    audio, sample_rate = librosa.load(PATH + 'clips/' + filename, sr=16000, duration = 10)\n",
    "    audio = padarray(audio, 160000)\n",
    "    return mfcc(audio, sample_rate, numcep=dim)\n",
    "\n",
    "def load_audio(filename):\n",
    "    audio, sample_rate = librosa.load(PATH + 'clips/' + filename, sr=16000, duration = 10)\n",
    "    audio = padarray(audio, 160000)\n",
    "    return audio, filename \n",
    "#     return np.zeros(160000), filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# audios preprocessing\n",
    "\n",
    "here I used MFCC feature extraction. mfcc dimension is 26. if you want to have 8000Hz you can use 13 as mfcc_dim\n",
    "\n",
    "create a list of audio paths. then make loop and calculate mfcc or psetrogram of all audios. array result shape (No. audios, time_seq, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy file\n",
    "train_features = [] # (No. audios, time_seq, feature)\n",
    "for path in train_attention['path']:\n",
    "    features_extracted.append(calc_mfcc(path))\n",
    "\n",
    "with open('features_extracted_train_attention.npy', 'wb') as f:\n",
    "    np.save(f, np.array(features_extracted))\n",
    "\n",
    "# create numpy file\n",
    "dev_features = [] # (No. audios, time_seq, feature)\n",
    "for path in dev_attention['path']:\n",
    "    features_extracted.append(calc_mfcc(path))\n",
    "\n",
    "with open('features_extracted_dev_attention.npy', 'wb') as f:\n",
    "    np.save(f, np.array(features_extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use next cell if you created npy file and you just want to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaad numpy file\n",
    "with open(PATH2 + 'features_extracted_train_attention.npy', 'rb') as f:\n",
    "    train_features = np.load(f)\n",
    "    \n",
    "with open(PATH2 + 'features_extracted_dev_attention.npy', 'rb') as f:\n",
    "    dev_features = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# captions Preprocessing and tokenizing the \n",
    "we have 2 approach :\n",
    "\n",
    "1. tf.keras.preprocessng.text.Tokenizer\n",
    "1. my own code ( we don't have it here. find this aproach in CTC_Custom_Generator folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "tokenized_captions = []\n",
    "# for annot in train_attention['sentence']:\n",
    "#     caption = '<start> ' + annot + ' <end>'\n",
    "#     all_captions.append(caption)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer( oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ', char_level=True, lower=True )\n",
    "tokenizer.fit_on_texts( list(dev_attention['sentence']) + list(train_attention['sentence']) )\n",
    "# tokenizer.fit_on_texts(train_attention['sentence'])\n",
    "train_seqs = tokenizer.texts_to_sequences(train_attention['sentence'])\n",
    "dev_seqs = tokenizer.texts_to_sequences(dev_attention['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'sentence : {train_attention[\"sentence\"][0]} , tokenized length : {len(train_seqs[0])}, original sentence length : {len(train_attention[\"sentence\"][0])} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_seqs[0])\n",
    "print(dev_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end to each sentence. add start and end to dict . add padding to dict and sentences. \n",
    "start_index_in_dict = len(tokenizer.word_index) + 1\n",
    "end_index_in_dict = len(tokenizer.word_index) + 2\n",
    "\n",
    "tokenizer.word_index['<start>'] = start_index_in_dict\n",
    "tokenizer.index_word[start_index_in_dict] = '<start>'\n",
    "tokenizer.word_index['<end>'] = end_index_in_dict\n",
    "tokenizer.index_word[end_index_in_dict] = '<end>'\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "def myfunc(x):\n",
    "    x.insert(0,start_index_in_dict)\n",
    "    x.insert(len(x),end_index_in_dict)\n",
    "    return x\n",
    "\n",
    "train_seqs = list(map(myfunc, train_seqs))\n",
    "dev_seqs = list(map(myfunc, dev_seqs))\n",
    "\n",
    "cap_vector_train = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "cap_vector_dev = tf.keras.preprocessing.sequence.pad_sequences(dev_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cap_vector_train[0])\n",
    "print(cap_vector_dev[0])\n",
    "print(cap_vector_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tf.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(train_features) // BATCH_SIZE\n",
    "# Shape of the vector extracted from mfcc is (999, 26), for spectrogram is (999, 166)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 26 # 166 for spectrogram\n",
    "attention_features_shape = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_features, cap_vector_train))\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention encoder decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Fun fact: the decoder below is identical to the one in the example for Neural Machine Translation with Attention.\n",
    "\n",
    "The model architecture is inspired by the Show, Attend and Tell paper.\n",
    "\n",
    "* In this example, you extract the features from mfcc giving us a vector of shape (999, 26).\n",
    "* This vector is then passed through the CNN Encoder (which consists of a single Fully connected layer).\n",
    "* The RNN (here GRU) attends over the frequencies to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 999, embedding_dim => mand dadam (256) )\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size).  hidden hidden state ghabli too balas.\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 999, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 999, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 999, embedding_dim (256))\n",
    "        self.fc = tf.keras.layers.Dense(256)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units ,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    \n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "    \n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), tf.expand_dims(x, 1)], axis=-1)\n",
    "#     print(x.shape)\n",
    "    \n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from audio to audio\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "  current_batch_size = img_tensor.shape[0]\n",
    "  dec_input = tf.expand_dims([float(tokenizer.word_index['<start>'])] * current_batch_size, 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(float(target[:, i]), 1)\n",
    "#           print(target[:,i])\n",
    "#           print(predictions[:, i])\n",
    "#           dec_input = predictions[:, i]\n",
    "        \n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitbef378172e7e4093bc71f9761c8616f0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}